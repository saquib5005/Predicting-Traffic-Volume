{"cells":[{"source":"![Traffic](traffic.png)\n\nTraffic data fluctuates constantly or is affected by time. Predicting it can be challenging, but this task will help sharpen your time-series skills. With deep learning, you can use abstract patterns in data that can help boost predictability.\n\nYour task is to build a system that can be applied to help you predict traffic volume or the number of vehicles passing at a specific point and time. Determining this can help reduce road congestion, support new designs for roads or intersections, improve safety, and more! Or, you can use to help plan your commute to avoid traffic!\n\nThe dataset provided contains the hourly traffic volume on an interstate highway in Minnesota, USA. It also includes weather features and holidays, which often impact traffic volume.\n\nTime to predict some traffic!\n\n### The data:\n\nThe dataset is collected and maintained by UCI Machine Learning Repository. The target variable is `traffic_volume`. The dataset contains the following and has already been normalized and saved into training and test sets:\n\n`train_scaled.csv`, `test_scaled.csv`\n| Column     | Type       | Description              |\n|------------|------------|--------------------------|\n|`temp`                   |Numeric            |Average temp in kelvin|\n|`rain_1h`                |Numeric            |Amount in mm of rain that occurred in the hour|\n|`snow_1h`                |Numeric            |Amount in mm of snow that occurred in the hour|\n|`clouds_all`             |Numeric            |Percentage of cloud cover|\n|`date_time`              |DateTime           |Hour of the data collected in local CST time|\n|`holiday_` (11 columns)  |Categorical        |US National holidays plus regional holiday, Minnesota State Fair|\n|`weather_main_` (11 columns)|Categorical     |Short textual description of the current weather|\n|`weather_description_` (35 columns)|Categorical|Longer textual description of the current weather|\n|`traffic_volume`         |Numeric            |Hourly I-94 ATR 301 reported westbound traffic volume|\n|`hour_of_day`|Numeric|The hour of the day|\n|`day_of_week`|Numeric|The day of the week (0=Monday, Sunday=6)|\n|`day_of_month`|Numeric|The day of the month|\n|`month`|Numeric|The number of the month|\n|`traffic_volume`         |Numeric            |Hourly I-94 ATR 301 reported westbound traffic volume|","metadata":{},"id":"8b752817-8333-446e-9790-73d85b0aa14f","cell_type":"markdown"},{"source":"# Import the relevant libraries\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader","metadata":{"executionCancelledAt":null,"executionTime":5583,"lastExecutedAt":1731178758169,"lastExecutedByKernel":"f1d3f3b0-11a7-4bca-ae45-aff28b9bdb0c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import the relevant libraries\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader","outputsMetadata":{"0":{"height":223,"type":"dataFrame"},"1":{"height":222,"type":"dataFrame"}}},"id":"d2e54daa-828a-420a-a204-f855de2ae375","cell_type":"code","execution_count":1,"outputs":[]},{"source":"# Read the traffic data from the CSV training and test files\ntrain_scaled_df = pd.read_csv('train_scaled.csv')\ntest_scaled_df = pd.read_csv('test_scaled.csv')\n\n# Convert the DataFrame to NumPy arrays\ntrain_scaled = train_scaled_df.to_numpy()\ntest_scaled = test_scaled_df.to_numpy()","metadata":{"executionCancelledAt":null,"executionTime":279,"lastExecutedAt":1731178758450,"lastExecutedByKernel":"f1d3f3b0-11a7-4bca-ae45-aff28b9bdb0c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Read the traffic data from the CSV training and test files\ntrain_scaled_df = pd.read_csv('train_scaled.csv')\ntest_scaled_df = pd.read_csv('test_scaled.csv')\n\n# Convert the DataFrame to NumPy arrays\ntrain_scaled = train_scaled_df.to_numpy()\ntest_scaled = test_scaled_df.to_numpy()","outputsMetadata":{"0":{"height":223,"type":"dataFrame"}}},"id":"58e3a3b9-9367-43e7-b13b-1f111447478e","cell_type":"code","execution_count":2,"outputs":[]},{"source":"# Write a function to create sequences from the data, returning NumPy arrays for modeling\n# This is commonly used for time series data, where you use a sequence of past values to predict the next value\ndef create_sequences(data, seq_length, y_col_idx):\n    \"\"\"\n    Generates input sequences and corresponding target values for time-series data.\n    Parameters: input data, number of time steps in each sequence, target column name.\n    Returns: NumPy arrays for input sequences an target values.\n    \"\"\"\n    xs, ys = [], []\n    for i in range(len(data) - seq_length):\n        x = data[i:(i+seq_length)]\n        y = data[i+seq_length, y_col_idx]\n        xs.append(x)\n        ys.append(y)\n    return np.array(xs), np.array(ys)\n\n# Create sequences for the training and test data\nX_train, y_train = create_sequences(train_scaled, 12, -1)\nX_test, y_test = create_sequences(test_scaled, 12, -1)\n\n# Create a TensorDataset for the training and test data for PyTorch modeling compatibility\ndataset_train = TensorDataset(\n    torch.tensor(X_train.astype(np.float32)).float(), torch.tensor(y_train.astype(np.float32)).float(),\n)\ndataset_test = TensorDataset(\n    torch.tensor(X_test.astype(np.float32)).float(), torch.tensor(y_test.astype(np.float32)).float(),\n)\n\n# Create a DataLoader for the training and test data, loading the data in batches and shuffling the data\n# Batch size 64 is a common choice to balance training speed and memory usage\n# Shuffle is True to reduce the risk of overfitting\n# Shuffle is False for test data following best practices\ndataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True)\ndataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=False)\n\n# Define an LSTM network\nclass TrafficVolume(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Define the LSTM layer\n        # 66 for the input_size is the number of features \n        # 64 is a common choice for hidden size\n        # 2 layers are selected to help learn more complex time-series patterns\n        self.lstm = nn.LSTM(\n            input_size=66,\n            hidden_size=64,\n            num_layers=2,\n            batch_first=True\n        )\n        # Define the activation function\n        self.relu = nn.LeakyReLU()\n        \n        # Define the fully connected layer\n        self.fc1 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        # Capture the final hidden state\n        _, (h_0, _) = self.lstm(x)\n        # Take the hidden state from the last layer\n        out = h_0[-1]\n        # Apply ReLU\n        return self.relu(self.fc1(out))\n    \n# Set-up for training \nn_features = 66\nhidden_size = 64\nnum_layers = 2\n\n# Initialize the model, saving it to traffic_model\ntraffic_model = TrafficVolume()\n\n# Define the loss function and optimizer\n# MSE is commonly used for regression tasks\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(traffic_model.parameters(), lr=0.0001)\n\n# Train the model with 2 epochs\nfinal_training_loss = 0\nfor epoch in range(2):\n    for batch_x, batch_y in dataloader_train:\n        optimizer.zero_grad()\n        outputs = traffic_model(batch_x)\n        loss = criterion(outputs, batch_y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(\"Epoch: %d, train loss: %1.5f\" % (epoch+1, loss))\n    final_training_loss = loss\n\n# Set the model to evaluation mode\ntraffic_model.eval()\n\n# Initialize variables to store outputs and labels\nall_predictions = []\nall_labels = []\n\n# Disable gradient calculation during inference\nwith torch.no_grad():\n    for seqs, labels in dataloader_test:\n        outputs = traffic_model(seqs).squeeze()\n        all_predictions.append(outputs)\n        all_labels.append(labels)\n\n# Concatenate all predictions and labels as PyTorch tensors\nall_predictions = torch.cat(all_predictions)\nall_labels = torch.cat(all_labels)\n\n# Calculate MSE directly with PyTorch\ntest_mse = F.mse_loss(all_predictions, all_labels)\n\nprint(f'Test MSE: {test_mse.item()}')","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":80,"type":"stream"}}},"id":"d29357d1-6cbb-4c86-ba49-ea31f6d8415e","cell_type":"code","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"Epoch: 1, train loss: 0.05776\nEpoch: 2, train loss: 0.06626\nTest MSE: 0.0734984427690506\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}